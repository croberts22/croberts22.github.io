---
layout: default
title:  "Interviewing Software Engineers Effectively"
author: "Corey Roberts"
permalink: /blog/interviewing-software-engineers-effectively.html
published: false
summary: "Interviewing for a company is hard. Being the interviewer can be harder. There are so many ways to approach a software engineering interview, from delivering a plethora of technical questions, to simulating team or company-based environments, to whiteboarding engineering problems with a subset of people. Given the wide number of vectors to gauge someone's competency and compatibility with your team, how can you be sure that you're interviewing someone effectively?"
---

Over the past handful of years, I've had amazing opportunities to conduct interviews for the teams I've been a part of. 

To be honest, this is a topic I've been meaning to write about for a long time. After reading blog posts from some of my former coworkers about scoring success when interviewing for a management or Android position, it made me want to write about the opposite perspective: how to interview the *interviewee*. I'm no expert at this, as I only base this on my personal experience and advice that I've been given by my colleagues. However, I hope that if anything, there's something in here that will help you consider how you interview and how to be more effective at building an amazing team! 

# Preparation

# Techniques
## Peeling the Onion

# Technical Questions

# Whiteboarding

Over the past five or so years, I've gone back and forth on whiteboarding questions. It was a long journey for me to figure out whether or not I enjoyed executing them in an interview. I ended up with the following conclusion on simulations:

> Whiteboarding is only effective if you can articulate and demonstrate your goals behind it.

Initially, I enjoyed them a ton because I thought of it as a great way to exercise one's technical prowess. This would be the time that someone could really show off their competency and breadth of knowledge. Because I wanted to see just how far someone could go, I would give a simple example. My go-to question was actually a derivative of a question that I was given when I interviewed at Evernote early 2012 (which I failed miserably at; a story for another post), which was the following:

> You are a TA for a math professor at a university, and for some reason, he wants you to create a program that can display a collection of shapes. Before we can create the GUI, we need to come up with a framework to support the creation of two-dimensional shapes. Create a framework that can support the following shapes: Circle, Square, Triangle. Additionally, create methods that can calculate the outline length (i.e. perimeter/circumference) and area of each of these shapes. 

I thought: if they could answer the question, then great! If not, then were they actually a fit for the role they were applying for? After all, this is "Computer Science 101": you've got easy opportunities to demonstrate conforming to a common interface, abstract interfaces, inheritance and overriding if we consider additional two-dimensional shapes like a rectangle or parallelogram; encapsulation if we wanted to conceal some of the properties of these shapes, and so on. Surely these were easy finds!

Oh, how naÃ¯ve I was. Out of all of the times I administered this question, about half of the candidates passed. The problem with my approach was actually quite simple: I was reflecting the ideas and actions that I had taken (which I had mulled over for days throughout the course of generating and administering this question), which only meant anything else that didn't fit this bill was invalid in my eyes. Now, I do believe there are inherently incorrect responses to these kinds of questions, and we often identify such responses as red flags (more on this later). However, the root problem I encountered was that I had a difficult time separating *my* optimal solution with a *candidate's* optimal solution, and that caused extreme bias in my feedback. 

After thinking about this suboptimal success rate, I discussed with a few engineering leaders about how they conducted interviews on their teams. One in particular abhorred whiteboarding for the reasons I had just reflected on:

> We often have extreme bias in our personal solutions to problems we've seen, comprehended, and solved in a much longer time limit than the candidate will *ever* get.

It definitely made me reconsider my approach to this question. This question was personal to me because of my failed experiences at Evernote and the desire to prove to myself that I could solve this optimally. While this question opened up numerous opportunities to talk about rudimentary computer science knowledge, I failed to peel that onion because the candidate's solution's shape (no pun intended, maybe ðŸ™‚) didn't resemble mine. And *that* was a huge failure on my end. 

For a while, I stopped doing these kinds of questions because I felt they were a bit unnecessary and quite frankly, overused. With the growth of the internet comes a diversity of whiteboard simulations (with solutions attached), so trying to give something overused like an anagram or string reversal question is like giving a free pass in the interview. So instead, we ended up replacing these with a two-hour programming block in hopes that we'd have the candidate be more comfortable with crafting a creative solution.

This also failed drastically. The success rate was even lower. But why was this? Well, we had almost set our candidate up for failure to begin with. While we had a two-hour block for the candidate to create their solution, we left them isolated in a small meeting room. Because engineers on the team had meetings to attend, it was usually difficult to have a two-hour block free for them to sit with the candidate. Thus, we'd encourage them to use Slack to discuss ideas with the team. They hardly utilized it. 

When we checked up periodically, often there was little code written due to sheer nervousness. And, at the end, the code would be hardly complete. It took me time to realize that the environment we put the candidate through was indicative of how flawed our interview process was, and how we failed to make it a representation of our team culture. Or, perhaps more candidly, it was likely *because* part of the team culture was focused on individuals working on projects, that this kind of environment felt very normal to us.

After thinking back on these incidents, I've bounced back to the idea of utilizing whiteboard questions. I've found the most success by applying two principles that I had failed to realize in both situations. My current philosophy has been the following:

> Whiteboarding questions should promote an environment that simulates a team discussing architecture ideas for a system and/or reviewing a pull request, with a goal to encourage and foster diverse ideas and solutions that fit the problem statement.




# Finding Your Red Flags 